AppBuilders Talk

INTRO

Hi! I’m Paris Buttfield-Addison, and I’m an iOS developer and game designer at Secret Lab. I’ve written a bunch of books for O’Reilly Media, and most recently released Practical Artificial Intelligence with Swift.

TASK-BASED MACHINE LEARNING

Machine learning has a reputation for being quite impenetrable, and a little bit scary at times. It's full of fancy words like activation function tensor generative adversarial networks mini batches stochastic gradient descent and many more and now it also has a reputation to be in very mathematics heavy it can be quite imposing and quite impressive to be working in machine learning and intensive. This can drive people away.

But it doesn't really need to be! The vast majority of really amazing machine learning power features can be built without you having to understand too much about what's going on under the hood. You don't need to have a deep understanding of the underlying technology, as long as you know the goals - that is, what you’re trying to accomplish. This is an approach called task based artificial intelligence.

There are certain tasks for machine learning is really quite good at. For example, image recognition, recommendations, sound classification, dictation, and other tasks that until reasonably recently were considered to be things that only human beings and their organic brains could do. These task and now achievable on small-scale devices like mobile phones through the application of technologies like deep convolutional neural networks and other words that give people who are scared of maths the heebie-jeebies. But again: we don't care about how the results are achieved, only that the results are achieved in the end.

The idea behind task based artificial intelligence is you build systems on top of existing much more detailed and much more technical machine learning infrastructure that is designed to support very specific practical tasks. You don't actually care how the system performs image classification for example you just care that you can feed and images and it can give you results. 

Knowing more information about how the machine learning system is forming this classification and generating results is absolutely useful but you don't need to know about it until the time comes for tuning and improving the accuracy and efficiency of your system. Until you are the point where that becomes important you do not need to care about the underlying implementation.

The machine learning technologies and frameworks that come with Xcode support this style of building machine learning into applications. While all of the underlying stuff is still exposed and you can make use of it, the tools also provide you with applications and frameworks to make it straightforward and simple to build machine learning models that can be applied to common tasks.

We only have a short amount of time together today, so we're going to take a look at two different kinds of machine learning features: big features, and small features.

First let's look at big features. These are the kinds of features where you build your entire application around the thing for example if you had a feature of your app that can tell you is a photo contains a fruit and if so what kind of fruit. 

Or you want to make an application to identify music. for example Shazam the music detection and classifying application that Apple purchased, can record audio and then identify the song that's being played in that audio. The entire application exists to present this feature to the user.

The other kind of feature is a small feature. Small features are where you use machine techniques to improve a facet of the application that you're building. 

For example, the built-in spellchecker and auto correction tool is able to identify domain specific terms such as iPad iPhone AirPods Apple Watch. Another example of a small feature is the face recognition system built into the Photos application on your Mac, iPhone, and iPad. The cool functionality of the photos app is to manage the content of your photo library, but it also has machine learning built in to identify the faces of the people in the photo. This improves the overall experience of the photos app, but the entire application is not built around this specific feature.

TOOLS

Today we going to look at both a big feature and a small feature. We're going to build an entire application built around a single task using machine learning, and we're going to add machine learning to improve a larger application. As part of this, you will learn how to use the existing machine learning developing tools that are already on your computer, and how you can get started with them today.

We are going to be using some common tools that are built in, and we're going to also download and install a couple of things that are not built-in. First we are going to use Xcode, the standard tool for building software the Apple platforms, we're going to use Create ML to train verify and export our model, and we're going to use Python to do some datasets preparation.

Xcode and Python are fairly commonly-known developer tools, so I'm not going to explain them in detail, but if you're not familiar with them then there's a lot of great resources available on Apple’s developer site, and the recordings of the talks at Apple developer conference.

Create ML is something that you might be a bit less familiar with. Create ML is an app that simplifies the creation of machine learning models are used for common machine learning tasks. 

You can create models that perform image recognition, text classification, as well as more generic tasks like item recommendation, and more. You simply choose what kind of machine learning task you'd like to perform, you feed it training data, and generate a model. It performs validation and testing of the model as well, and it's really good at the tasks that we have in mind today! It comes built-in with Xcode, and requires macOS 10.15 Catalina.

A SOUND CLASSIFIER

The first thing that we're going to do is build a application that can classify sounds. It will be able to listen to the outside world, and identify what type of animal can be heard in that audio. Once it's determined what kind of animal we are listening to, it will then show an icon on the screen.

There are a few things we have to do in order to build this application. First, we need to decide what kind of task we’re performing, and see if the existing tools that we are working with can handle this kind of task. Next we need to acquire a dataset that we can use to train the machine learning model. We’ll then need to prepare the data for training, train the model, and then build an application that can feed data into the model and get back results.

This specific kind of task we’re trying to do is a sound classification task. Given a chunk of recorded audio, we want to feed that audio into a pacifier, and get back a set of results. Each of these results will be the type of sound that we have identified, and how confident we are in applying that label to the recording. 

 To build a model, we need to decide on what possible labels we want to apply. For this application, we care about animals. We'll decide on a list of animals that we want to train the model to recognise. One side effect of this is that if we have audio of any other kind of animal, we won't be able to recognise correctly. In fact, it will mislabel the recording as one of the animal types we specified. There are ways to work around this, but we’ll accept the limitation in this demo.

The model will only recognise sounds that it was previously trained on. This means that whatever kind of data we give it during training will form the list of possible labels that we can apply to incoming audio. 

This is why it's very important to have a large and diverse training dataset, and it's also the reason why large companies like Google spend a great deal of effort and money collecting human verified data. If you've ever completed a CAPTCHA hosted by Google, you've provided them with some human verified labelling information for data. 

This is why those kinds of tests generally include tasks like identifying street signs or vehicles: it's for training object classification and object recognition in machine learning models.

We don't have time to put together our own training data set today, because it's a fairly large task. Fortunately, there are datasets out there that we can make use of. One of these is ESC–50. This is a labelled collection of 2,000 environmental audio recordings taken from the FreeSound project. The recordings are of things like animals natural sounds human noises and more.

To get started, we’ll download the dataset from GitHub. It's about 850 MB. Once you’ve downloaded it, unzip it somewhere. We won’t be using all of the data, just the animal noises.

The full dataset is licensed under the creative Commons attribution non-commercial license, which means you can't use it for commercial use. They do have a smaller 10 label dataset that is licensed as Creative Commons Attribution.

Now that we’ve downloaded the dataset, we need to prepare the data for Create ML to work with it. Create ML expects the files to be sorted into folders, and each folder becomes the label for those files. For example, all of the recordings of dogs need to be in a folder named ‘dogs’, all of the cats in a folder named ‘cats’, and so on. 

I’ve moved each of the files so that they’re in a folder that matches their label.

<OPEN CAT FOLDER>

If I open the ‘cat’ folder, all of the files are recordings of cat noises.

Now that we've prepared our data for training, we're ready to create the model using Create ML. Open Create ML by opening the Xcode menu, choosing Open Developer Tool, and choosing Create ML.

<OPEN CREATE ML>

Click New Document and then choose Sound Classifier from the list of templates.

<CLICK NEW DOCUMENT>

<CHOOSE SOUND CLASSIFIER>

Click next, and save the project somewhere. 

<SAVE PROJECT> 

Next select the model source from the list on the left-hand side of the window. Open the drop-down menu in Training Data, and select Select Files.

<SELECT MODEL>

<SELECT TRAINING DATA>

Locate the classes folder that the script generated a moment ago.

<SELECT FILES>

Click Open.

<CLICK OPEN>

Create ML will split these files into a training set and a validation set. We can provide our own validation set if we want to, or we can leave it set to “automatic"; in this mode, Create ML will divide the training data into the two sets randomly.

Create ML will detect that you have 10 labels, and a total of 400 items. We’re now ready to train.

Click the train button, and watch as Create ML trains the model. 

<CLICK TRAIN>

It will first start extracting features from the audio, and then it will begin multiple iterations of training. For a small dataset like this, it won't take very long to train. On my machine, it takes about two minutes to finish.

Once it’s done training, you can take a look at how well the finished model performs. Create ML will show what percentage of the training and validation data the model was able to correctly recognise. 

We can also run tests on the model by providing it with additional labelled sounds that the model has never seen before.

The model is now ready to use. Let’s drag the file into a folder.

<DRAG FILE INTO FOLDER>

THE BASE APPLICATION

Before we start working with the trained model, let’s look at the base application that we’re going to put the model in.

The app has a collection of icons, one for each of the ten types of animal that we can recognise. When we tap the Record button at the bottom of the screen, it will record a few seconds of audio, and save it to a file.

The part that’s missing is the classification. We want to give the recorded sound file to the model, and get back which type of animal it’s decided to label it as. Once we have that, we’ll highlight the corresponding animal icon.

USING MODELS

The first thing we need to do is add the trained model to the app. I’ll do this now, by dragging and dropping it from the folder I put it in.

<DRAG MODEL INTO PROJECT>

A CoreML model is just a data file. In order to use it in our app, we need code that can open the file, and work with it. Fortunately, Xcode can generate it for us.

<OPEN GENERATED SOURCE CODE>

The generated model class allows us to feed inputs in, and get outputs back. We won’t be using it directly, because we’ll be using sound classifier classes that make it easier to take audio data and get back results. 

That said, it’s worth looking at the code that Xcode generates, because it’s useful to understand how the model works with the rest of your code.

When we open the model in Xcode, the Model Class section gives us a link to the generated source code for a class that can work with the model. So, that was easy.

Sound isn’t an instantaneous thing. It’s something that happens over time. If you’re analysing live audio, there’s never a moment that the analysis has “finished” and generated a single result. 

Because of this, the sound classification libraries that come with Core ML will actually generate multiple results over time, when you give it audio data. 

We now need to think about how we provide audio data to the model. CoreML models work based on requests: when you want to give data to the model, you create a request, and submit it. After the model has been run, you get back results. 

Each result contains two important values: the label that the model wants to apply to the data, and how confident the model is. 

To get results, we’ll create an SNAudioFileAnalyzer. This is a class that takes a file, and notifies another object about the results that it’s seen. This other object is called the observer, and it receives the requests as they come in. It’s also notified when the analysis is complete. The observer conforms to the protocol SNResultsObserving.

When a result arrives, the method “request didProduce result” is called. Each time we receive a result, we’ll keep it, if it has a reasonably high degree of confidence. When all of the audio data has been analysed, we’ll look at the accumulated results, and let the application know which result we’re most confident about. This will then be used to indicate which animal icon to light up.

Let’s get started adding the audio analysis features to this app. I’ve created an empty file called Audio.swift, where I’ll put all of my analysis code, so that I don’t need to touch the view controller code very much.

<OPEN Audio.swift>

I’ll start by importing the CoreML, AVFoundation and SoundAnalysis modules, which contain the classes I need.

<CODE: 1. Import modules>

Next, I’ll create a new class, called AudioClassifier, which manages the ML model and the sound classification code. It will instantiate the model class, as well as a sound classification request that uses the model. 

<CODE: 2. Define AudioClassifier>

Notice that the classification request has a ‘try’ - that’s because initialising it might fail if we used a model that wasn’t designed for audio.

We also need to create the results observer class, which will receive information from the model, and store the results.

<CODE: 3. Define ResultsObserver>

We have some important methods to add. First up, we’ll write the “request did produce result” method, which will be called every time the analyser produces a classification result. It will take the result, check to see if it’s confident enough, and if it is, store it in the results array.

<CODE: 4. request did produce result>

Next, we’ll write the “request did fail with error” method. As you might guess from the name, it’s called when the analyser fails. When this happens, we’ll return a nil result to the rest of the application.

<CODE: 5. request failed>

Finally, we’ll add the “request did complete” method, which is called when we’re all done with the analysis. In this method, we’ll select the label that we have the highest confidence in, and return that to the application.

<CODE: 6. request did complete>

Now that we’ve written the request observer class, we can use it. We’ll go back to the AudioClassifier class, and add some code that creates an audio file analyser, and gives it an audio file to recognise, as well as a request observer to notify when it gets results. We’ll create the analyser, add the request, and then tell it to analyse.

<CODE: 7. Perform analysis>

That’s all of the machine learning code we need to do! Now, we just need to attach it to the user interface. We’ll go back to ViewController.swift.

<OPEN ViewController.swift>

We don’t have much to do here. First, we’ll create an instance of the AudioClassifier, as a property.

<CODE: 8. Add classifier object>

Finally, in the classifySound method, which is called when the recording completes and receives the URL to the audio file, we’ll take the string that we received, convert it to an animal, and let the view controller know. We’re done!

<BUILD AND RUN THE APP>

We can now test it! I’ve got some recordings of animal noises here. Let’s see how they work.

<TECH: ENSURE PHONE SCREEN IS BEING RECORDED>

<PLAY ANIMAL NOISES, DEMONSTRATE APP>

And there we have it! Our animal noise detector app!

SMALL FEATURES: AUTOMATIC CAPTION GENERATION

Now that we’ve built an app around a single machine learning feature, let’s look at how we can use machine learning to improve a larger app. While it’s interesting to build a cool toy that shows off the power of CoreML, machine learning is at its best when it’s improving the user experience of an app.

So, let’s improve an app using machine learning. I’m about to launch my amazing, brand new, and totally original social network site, called Birder. On Birder, users can post “chirps”, which are short messages. These messages can have images attached.

It’s really important that my app be accessible to users with limited vision, so I’m designing Birder so that users can add captions that describe the images that they’re posting. This is important for screen readers, who will use the caption when reading the text to the user.

I’ve already added the ability to take photos and add captions. If I launch the app and take a photo, I can type in a description of what’s in the photo.

<LAUNCH APP, TAKE PHOTO>

But it’s easy to just skip that step. I could make it mandatory, but it’s important to remember that the more steps you add increase friction. Another idea is to use machine learning to get us part of the way there. We can use CoreML to do object recognition on the photos that we add, and use that to create a starting point for our captions. This doesn’t mean that we’re absolving users of the need to add descriptive text, but it does reduce the amount of work.

There are two common types of image analysis: object classification, and object detection. Object classification models attempt to classify the dominant object in an image, while object detection models attempt to identify multiple objects in an image. Classification models answer the question, “what object is this an image of?”, while detection models answer the question, “what’s in this image?”. It’s a subtle difference, but important. 

Because we want to answer the question “what’s in this image”, we’ll use a classification model. A good one is YOLOv3Tiny. YOLO stands for “you only look once”, and it was developed by Joseph Redmon and Ali Farhadi . You give it an image, and it returns information about which regions of the image contain an object that it’s recognised, and possible identifiers for those objects. It’s called ‘tiny’ because it’s designed to be very small - the model is only 35MB, and less accurate ones are even smaller. 

A pre-trained YOLOv3Tiny model is available for download from Apple’s CoreML models site. I’ll go ahead and download that now.

Because we’re generating captions for our images, we don’t really care about where the objects are in the image, just what objects are in the image. This means that we’re being slightly inefficient with our use of this model, but it’s fine for this demo.

Let’s get started in adding object detection to Birder. We’ll start by adding the downloaded YOLOv3Tiny model to the project.

<DRAG MODEL INTO PROJECT>

<OPEN PostChirpViewController.swift>

In order to submit images to the model, we need to process them. The model is expecting a 416 by 416 square grid of colours, but the images that come off the camera will be bigger. They need to be scaled and cropped. Fortunately, iOS comes with a framework that takes care of this for us. It’s called Vision, and I’ve already imported it at the top of my code, along with CoreML.

We deliver images to the model by creating a request. A request delivers the information to a model, and calls a completion handler when the model has finished processing. The request also has properties that let us set up how the image should be delivered, like how it should be scaled and cropped. Let’s add that now.

<CODE: 1. Create observation request>

The observation request uses a CoreML model class. In this case, we’ll use the YOLOv3Tiny class that Xcode generates for us. It also uses a completion handler, which is called when the request generates a result. For this app, the completion handler will call the processObservations method, which will use the results to generate and display the caption.

Now that we’ve set up our request, we need to use it when the user takes a photo. My code has already been set up so that when an image is taken, it calls imageWasCaptured. At the moment, all it does is display a placeholder caption. To use this image with the request, we need to create a handler, and ask the handler to perform the request. 

<CODE: 2. run request with handler>

We’ll take the image, and use it to create a handler. We also need to get its orientation, because the handler uses that information to make sure that the image is sent to the model the right way up. Finally, you might notice that we’re doing all of this on a background dispatch queue, so that the user interface thread doesn’t get blocked while the model is working. When the request is complete, processObservations will be called.

<CODE: 3. process observations>

When process observations is called, it receives either an array of results, or an error. To use the results, we cast it to an array of VNRecognizedObjectObservation objects - this is a safe operation, because the model is guaranteed to only return results of this type - and then we can use the results to produce the caption.

<CODE: 4. get labels for observations>

Each result represents an object that the model detected in the image. The model can recognise 80 different categories of object, and for each result, we get the confidence for each category. What we want to do next is select the label that has the highest confidence. We also only want to use that label if we’re at least 80% confident in it. This means that if we take a photo, and none of the objects have a label with enough confidence, then we won’t generate a caption. It’s better to give no result than give a bad result.

We’re all done! Let’s test it out.

<LAUNCH APP, TAKE PHOTO>

Now, when I take a photo of an object, it generates a caption for me.

How to use machine learning well

It’s important to remember that machine learning isn’t a shortcut, and it’s not a substitute for having a human being in the loop. When we generate our caption for Birder, we make sure that the user has a chance to edit, improve or remove it before it gets posted.

Machine learning technologies are not really performing cognition. They’re generating automated results that don’t take into account any positive or negative impact on people. What they’re REALLY good at is getting you part of the way there, at very low cost and at scale.

To see what I mean, let’s compare a human to a machine learning system, when it comes to image recognition problems. When it comes to speed, a human is slow, but a machine is fast. When it comes to cost, a human is expensive, but a machine is cheap. When it comes to scalability, hiring more humans is slow, but spinning up more machines is fast. So much for humanity.

But it’s not all good news for the robot overlords. When it comes to bias, a human has lots, but a computer has MORE, due to it being a lot less flexible after training - decisions at training time are encoded, and it can only recognise classes that it was trained on -  far less than are available to a human. For quality, a human being is generally excellent, regardless of educational level, while a machine is OKAY on average. Finally, people are affected by the decisions that are made based on a result, while a machine isn’t. This fact alone means that purely AI-driven decisions are a terrible idea.

Machine learning based solutions are at their best when they’re used to get a task part of the way there, and let a human be the final judge. In this talk, I hope I’ve given you a taste of what you can do with these technologies, and gotten you excited to apply them in your next app.

You can follow me on Twitter at parisba, and I’d love to hear from you about what you think! 

TBD PARIS: WHAT DOES POST-TALK Q&A LOOK LIKE?